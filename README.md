# Codebase Analyzer Tool - CAT

Tools to analyze and capture metadata from an existing application's codebase into a datastore and then derive higher-level insight about the application. The tools let you generate a report describing the application's purpose and composition. A tool is also provide to allow you to ask ad hoc questions of the codebase. Where applicable, the tools use AI LLMs to help accelerate insight generation. The tools rely on the use a of MongoDB Atlas database (with its Vector Search capability) to store and retrieve insights.

The following screenshot shows part of an example report generated by these tools.

![Generated Example Report](docs/readme/report.png)

TODO:
You can also [view an example of a full report generated](https://fuzzy-robot-r4ppqny.pages.github.io/example/report.html) by the tools.


## Prerequisites

1. Ensure you have the following software installed on your workstation:

    - [Node.js JavaScript runtime](https://nodejs.dev/en/download/package-manager/)
    - [`npm` package manager CLI](https://docs.npmjs.com/downloading-and-installing-node-js-and-npm)
    - [TypeScript npm package](https://www.npmjs.com/package/typescript)
  
1. Install the project's required Node/NPM packages. 

    ```console
    npm install
    ```

1. Ensure you have the codebase for a sample application ready to access on your local filesystem. You can also [download a zip of some example projects](https://drive.google.com/file/d/1rDSOiLOH0xq3Hc5k8i3DvZpCDVvDccr1/view?usp=sharing) for testing. Note the current version of these tools work better with Java-based codebases, but over time, many more programming languages will be supported equally.

1. Ensure you have can leverage LLMs from OpenAI/Azure GPT, GCP Vertex AI or AWS Bedrock API, with the following three models types available to use, along with appropriate API keys / credentials:

    -  __Embeddings model__ for generating vector embeddings 
    -  __Text completions 'primate'model, typically with a small token limit__ for generating text and JSON content for dealing with text inputs 
    -  __Text completions 'secondary' model, typically with a large token limit__ for generating text and JSON content for dealing with text inputs (acting as a backup in case the primary model errors for a particular piece of cotent)

1. From the root folder of this project, run the following command to copy an example environment configuration file to a new file into the same root folder called `.env`:

    ```console
    cp 'EXAMPLE.env' '.env'
    ```

    Edit the `.env` file to:
    - Set your `LLM` provider (e.g., "OpenAI", "VertexAIGemini", etc.)
    - Set your MongoDB URL and codebase directory path
    - Add the specific environment variables required for your chosen LLM provider
    
    The system uses a **manifest-driven approach** - you only need to configure environment variables for your selected LLM provider. The application will automatically validate only the variables required for your chosen provider and provide clear error messages if any are missing. See the section [Application to LLM Authentication And URN Notes](#application-to-llm-authentication-and-urn-notes) for help on determing the correct URNs for you to specify in the `.env` file.

1. Ensure you have a running MongoDB [Atlas](https://www.mongodb.com/atlas) version 7.0 or greater dedicated cluster of any size/tier. You can even use an 'M0' free-tier version, although for some uses cases, the free-tier storage limit of 512MB may be insufficient. Ensure the approprate network and database access rights are configured. Optional because some use cases won't neeed a database. 


## Easy Debugging

There are various tools to be run in order (shown the next section). For each tool, it very easy to debug using VS Code and by following these steps:

1. Open the project in VS Code
1. In the _Explorer_ select the "src/cli/*.ts" file you want to run
1. From the _Activity Bar_ (left panel), select the _Run and Debug_ view
1. Execute the pre-configured task _Run and Debug TypeScript_
    - this will run the TypeScript compiler first, and then, if successful, it will run the program in debug mode, showing its output in the _Debug Console_ of the _Status Bar_ (bottom panel). 

Alternatively, you also run the `./dist/src/cli/c*.js` JavaScript files (first compiled from TypeScript using the `npm build` command) from the terminal using the `node` command. The command to run each tool is shown the next section.


## How To Run Main Tasks

(for a summary of the purpose of these tools, see the section [Analysis And Summary Capture Process](#analysis-and-summary-capture-process))

1. **BUILD THE PROJECT SOURCES**: First build the project (compiles TypeScript and moves some HTML template files to project's executable path) by executing the the following command.

    ```console
    npm run build
    ```


1. **CAPTURE SOURCES**: To capture LLM-generated metadata about every source file into the database, execute the the following commands (builds the project first) from a terminal (or select the corresponding "src/cli/*.ts" file in your VS Code and choose to "Run and Debug).

    ```console
    node ./dist/src/cli/capture-codebase.js
    ```

    Note 1. If you are getting LLM provider authentication/authorisaton errors when you executed the task, see the section [Application to LLM Authentication And URN Notes](#application-to-llm-authentication-and-urn-notes) for help on configuring LLM provider credentials correctly.

    Note 2. The taks will take around 10 minutes or more to execute, depending on the complexity of the source project. This tool employs asynchronous IO and concurrency, but inevitably, the LLM you provide to the tool will take time to respond to requests and often apply throttling, which will be the main causes of slowdown. If you see messages with the character `?` in the tool's output, this indicates that the LLM is returning an "overloaded" response, and hence, the tool will transparently pause each affected LLM request job and then retry after a short wait.


1. **GENERATE INSIGHTS**: Run the following command to generate insight (e.g. identifid technology stack, business processes, DDD aggregates, potential microserices, etc.) based on the previously database captured sources files metadata.

    ```console
    node ./dist/src/cli/capture-insights.js
    ```

1. **CREATE REPORT**: Run the following command to generate a static HTML-based report summarising the application from the previously captured source metadata and aggregated insights. 

    ```console
    node ./dist/src/cli/create-report.js
    ```

## OPTIONAL: How To Run Any Additional Tasks

1. **QUERY CODEBASE**: To adhoc query the codebase or "talk to your code" (which uses MongoDB's Vector Search capability to search the database-captured metadata), place your questions in the file `input/questions.prompts` and then run the following command to execute the queries. 

    ```console
    node ./dist/src/cli/query-codebase.js
    ```

1. **OTHER TOOLS**: You wil notice other tools are also provided in the "src/cli" folder which you can explore and run. These are currently undocumented and may disappear in the future are primarily for use by the project's developers.


## Running The Project's Full Build Process Including RUnning Unit And Integration Tests

Execute the following command from the project's root folder:

  ```console
  npm run validate
  ```


## Application to LLM Authentication And URN Notes

In all cases, first ensure you have enabled the required model for the requried cloud region using the cloud provider's LLM configuration tool.

### OpenAI / Azure GPT

Specify your API key for your own OpenAI or Azure GPT service in `.env`.


### GCP Vertex AI

```console
gcloud auth login
gcloud auth application-default login
```

### AWS Bedrock

Use MDB MANA to access AWS accounts to obtain the SSO start URL, then using the AWS CLI run:

```console
aws configure sso
```

Then edit the file `~/.aws/config` and rename the line `[profile ...]` for the newly generated profile section to `[default]` instead, then run:

``` console
aws sso login
aws sts get-caller-identity        # to test the CLI works
```

For later Bedrock hosted models, need to use the ARN of an inference profile for the particlar region for the model id. To see the region ARNs for models in Bedrock, run:

```console
aws bedrock list-inference-profiles
```

From this output, use the URL defined for the `inferenceProfileArn` parameter.

In the AWS Console, select the Bedrock Configuration | Model Access option and enable access for the models requried.


## Analysis And Summary Capture Process

The `capture-codebase` tool captures metadata about all the files in the codebase and AI-generated summary data in a series of collections in the `codebase-analyzed` database. The main steps this tool conducts are:

1. Initialise database collections and indexes (including Atlas Vector Search indexes).

1. Loop through every nested file in the referenced codebase, capturing the following into the database collection `sources`:
    - the file's content (e.g., code)
    - the file's metadata (e.g., file type, filepath)
    - LLM-generated summary data for the specific file type (e.g., if the file is a Java source, captures information about the the class, including its public methods with signatures, other classes referenced by this class, etc.)
    - two separate vector embeddings, indexed via Atlas Vector Search (one for the file's content and one for the file's LLM-generated summary).
    
    Below is an example of a record stored in the database representing an introspected Java file:

    TODO
    ![Generated Example Record](docs/readme/record.png)

Using metadata about the source files that was captued the `sources` collection, the subsequent `capture-insights` tool uses an LLM to generate insights of various aspects of the application persisted into the database collection `appsummaries`. This captured information includes an outline of the application's purpose plus a list its technologies, business processes, DDD bounded contexts, DDD aggregates, DDD entities,, DDD repositories, and potential microservices.


## Demonstrated LLM Capabilities For This Project

Tested on 17-July-2025 with the legacy Java Petstore J2EE application:

| LLM Hosting/API Provider | LLMs | Insight Quality <sup>1</sup> | Speed <sup>2</sup> | Average Error Rate <sup>3</sup> |
| :---- | :---- | :---: | :---: | :---: |
| Azure OpenAI | GPT4o | 4 | 1:47 mins | 0.4 % |
| GCP VertexAI | Gemini 2.5 Pro \+ Flash | 5 | 13:44 mins | 0.6 % |
| AWS Bedrock | Claude Sonnet 4.0 & 3.7 | 4 | 23:47 mins | 1.1 % |
| AWS Bedrock | Amazon Nova Pro 1.0 Pro \+ Lite | 3.5 | 9:30 mins  | 0.8 % |
| AWS Bedrock | Meta Llama Instruct 3-3-70b \+ 2-90b  | 2 | 39:53 mins | 43 %  |
| AWS Bedrock | Mistral Large 2402 \+ 2407 | 2 | 15:12 mins | 24 % |
| AWS Bedrock | Deepseek R1 | 3.5 | 10:12 mins | 1 % |


1. **Insight Quality**: 1 - 5, 1 = Low, 5 = High

1. **Speed**: Time taken to extract insights from all source files incluing for waits/restries

1. **Error Rate**: Proportion of requests that the LLM could not process even after retries or where the prompt had to be truncated to fit the LLM's context window


## LLM Routing Abstraction

The `LLMRouter` class (see `src/llm/llm-router.js`) performs various tasks to enable the LLM implementations, with their various APIs, to be pluggable, abstracting away many of the undesirable non-functional behaviours which occur when using LLMs. These non-functional features include:

* **Pluggable LLM**.  Routes to an LLM with a specific type and token size limit.
* **Automatic LLM Timeouts**.  Proactively times out LLM invocations if the LLM is taking too long to respond.
* **Automatic LLM Retries**.  Detects throttling and timeout events from the LLM and automatically retries for several attempts, with pauses in between.
* **Enforce JSON Response From LLM**.  Enforces a JSON format response, validated against a prescribed JSON schema, and automatically retries if a response is invalid JSON.
* **Automatically Fails Over To A Backup LLM**.  If the primary LLM's API response is consistently struggling to correctly process a particular prompt (e.g. repeated timeouts, repeated token limits exceeded), it automatically sends the request to a secondary LLM from the same family (if one has been specified).
* **Reactively Crop Excessively Large Prompt**.  If the LLM response error indicates the prompt is too big, shortens it to the size instructed by the LLM's API response, if specified; otherwise, estimates the appropriate shorter size to apply - note, it only crops after first trying the full prompt with the backup secondary LLM first, in case that has a larger accomodating limit).
* **Provide LLM Invocation Statistics**.  Gathers LLM invocation statistics (successes, failures, retries, truncations, etc.), logging each event and then providing a final summary table of totals at the end of the run.
